import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'  # è®¾ç½®é•œåƒæº

import logging
from typing import List, Dict, Tuple, Optional
import numpy as np
from pathlib import Path
from huggingface_hub import snapshot_download

# ç¬¬ä¸‰æ–¹åº“
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from rank_bm25 import BM25Okapi
from transformers import AutoModelForSequenceClassification, AutoTokenizer
import torch

class RAGRetriever:
    """RAGæ£€ç´¢å™¨ï¼ˆæ”¯æŒå®Œæ•´RAGæµç¨‹ï¼šå¤šè·¯å¬å›+æ··åˆæ£€ç´¢+é‡æ’åºï¼‰"""
    
    def __init__(
        self,
        vector_db_path: str,
        embedding_model_path: str = "./model/embeddingmodel",
        rerank_model_name: str = "./model/reranker",
        device: str = "cpu",
        download_mirror: Optional[str] = None
    ):
        """
        åˆå§‹åŒ–æ£€ç´¢å™¨
        :param vector_db_path: å‘é‡æ•°æ®åº“è·¯å¾„
        :param embedding_model_path: æœ¬åœ°åµŒå…¥æ¨¡å‹è·¯å¾„
        :param rerank_model_name: é‡æ’åºæ¨¡å‹åç§°æˆ–è·¯å¾„
        :param device: è®¡ç®—è®¾å¤‡
        :param download_mirror: æ¨¡å‹ä¸‹è½½é•œåƒåœ°å€
        """
        self.device = device
        self.download_mirror = download_mirror
        self._init_logging()
        
        # åˆå§‹åŒ–åŸºç¡€ç»„ä»¶
        self._load_embedding_model(embedding_model_path)
        self._load_vector_db(vector_db_path)
        self._load_rerank_model(rerank_model_name)
        
        # åˆå§‹åŒ–BM25
        self._init_bm25()

    def _init_logging(self):
        """é…ç½®æ—¥å¿—è®°å½•"""
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(levelname)s - %(module)s - %(message)s"
        )
    
    def _load_embedding_model(self, model_path: str):
        """åŠ è½½æœ¬åœ°åµŒå…¥æ¨¡å‹"""
        try:
            self.embedding_model = HuggingFaceEmbeddings(
                model_name=model_path,
                model_kwargs={'device': self.device},
                encode_kwargs={'normalize_embeddings': False}
            )
            logging.info("âœ… åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            logging.error(f"âŒ åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            raise
    
    def _load_vector_db(self, db_path: str):
        """åŠ è½½å‘é‡æ•°æ®åº“"""
        try:
            self.vector_db = FAISS.load_local(
                folder_path=db_path,
                embeddings=self.embedding_model,
                allow_dangerous_deserialization=True
            )
            logging.info(f"âœ… å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸï¼ˆ{self.vector_db.index.ntotal}æ¡æ•°æ®ï¼‰")
        except Exception as e:
            logging.error(f"âŒ å‘é‡æ•°æ®åº“åŠ è½½å¤±è´¥: {str(e)}")
            raise
    
    def _load_rerank_model(self, model_name: str):
        """åŠ è½½é‡æ’åºæ¨¡å‹ï¼ˆè‡ªåŠ¨ä¸‹è½½å¦‚æœä¸å­˜åœ¨ï¼‰"""
        try:
            # æ£€æŸ¥æ˜¯å¦ä¸ºæœ¬åœ°è·¯å¾„
            model_path = Path(model_name)
            if not model_path.exists():
                self._download_rerank_model(model_name)
            
            self.rerank_tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.rerank_model = AutoModelForSequenceClassification.from_pretrained(model_name)
            self.rerank_model.to(self.device)
            logging.info(f"âœ… é‡æ’åºæ¨¡å‹åŠ è½½æˆåŠŸ: {model_name}")
        except Exception as e:
            logging.error(f"âŒ é‡æ’åºæ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            raise
    
    def _download_rerank_model(self, model_name: str):
        """ä¸‹è½½é‡æ’åºæ¨¡å‹"""
        try:
            logging.info("å¼€å§‹ä¸‹è½½é‡æ’åºæ¨¡å‹...")
            
            # è®¾ç½®é•œåƒç¯å¢ƒå˜é‡(å¦‚æœæä¾›)
            if self.download_mirror:
                os.environ['HF_ENDPOINT'] = self.download_mirror
            
            # ç¡®ä¿æ¨¡å‹ç›®å½•å­˜åœ¨
            model_dir = Path(model_name).parent
            model_dir.mkdir(parents=True, exist_ok=True)
            
            snapshot_download(
                repo_id=model_name,
                local_dir=str(model_dir),
                resume_download=True
            )
            logging.info(f"âœ… é‡æ’åºæ¨¡å‹ä¸‹è½½å®Œæˆ: {model_name}")
        except Exception as e:
            logging.error(f"âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥: {str(e)}")
            raise
    
    def _init_bm25(self):
        """åˆå§‹åŒ–BM25ç´¢å¼•"""
        try:
            all_texts = [doc.page_content for doc in self.vector_db.docstore._dict.values()]
            self.bm25_index = BM25Okapi([doc.split() for doc in all_texts])
            self.all_texts = all_texts
            logging.info(f"âœ… BM25ç´¢å¼•æ„å»ºæˆåŠŸï¼ˆ{len(all_texts)}æ¡æ•°æ®ï¼‰")
        except Exception as e:
            logging.error(f"âŒ BM25ç´¢å¼•æ„å»ºå¤±è´¥: {str(e)}")
            raise

    def multi_retrieval(self, query: str, top_k: int = 10) -> Dict[str, List[Tuple[str, float]]]:
        """
        å¤šè·¯å¬å›æ£€ç´¢
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param top_k: æ¯è·¯å¬å›æ•°é‡
        :return: å„è·¯çš„æ£€ç´¢ç»“æœå­—å…¸
        """
        results = {}
        
        # å‘é‡æ£€ç´¢
        vector_results = self.vector_db.similarity_search_with_score(query, k=top_k)
        results["vector"] = [(doc.page_content, score) for doc, score in vector_results]
        
        # BM25æ£€ç´¢
        tokenized_query = query.split()
        bm25_scores = self.bm25_index.get_scores(tokenized_query)
        bm25_indices = np.argsort(bm25_scores)[::-1][:top_k]
        results["bm25"] = [(self.all_texts[i], bm25_scores[i]) for i in bm25_indices]
        
        logging.info(f"ğŸ” å¤šè·¯å¬å›å®Œæˆï¼šå‘é‡å¬å› {len(results['vector'])} æ¡ï¼ŒBM25å¬å› {len(results['bm25'])} æ¡")
        return results

    def hybrid_search(self, retrieval_results: Dict[str, List[Tuple[str, float]]], 
                      weights: Dict[str, float] = None) -> List[Tuple[str, float]]:
        """
        æ··åˆæ£€ç´¢ï¼ˆåŠ æƒèåˆï¼‰
        :param retrieval_results: å¤šè·¯å¬å›ç»“æœ
        :param weights: å„æ£€ç´¢æ–¹æ³•çš„æƒé‡ï¼ˆé»˜è®¤ï¼šå‘é‡0.6ï¼ŒBM25 0.4ï¼‰
        :return: èåˆåçš„ç»“æœåˆ—è¡¨
        """
        # è®¾ç½®é»˜è®¤æƒé‡
        default_weights = {"vector": 0.6, "bm25": 0.4}
        weights = weights or default_weights
        
        # ç»“æœèåˆ
        fused_results = {}
        for method, docs in retrieval_results.items():
            for doc, score in docs:
                # åˆ†æ•°å½’ä¸€åŒ–å¤„ç†
                norm_score = 1 / (1 + np.exp(-score)) if method == "vector" else score
                fused_score = norm_score * weights.get(method, 0)
                if doc in fused_results:
                    fused_results[doc] += fused_score
                else:
                    fused_results[doc] = fused_score
        
        # æ’åºå¹¶è¿”å›
        sorted_results = sorted(fused_results.items(), key=lambda x: x[1], reverse=True)
        logging.info(f"ğŸ§¬ æ··åˆæ£€ç´¢å®Œæˆï¼šèåˆ {len(sorted_results)} æ¡ç»“æœ")
        return sorted_results

    def rerank(self, query: str, documents: List[str], top_k: int = 5) -> List[Tuple[str, float]]:
        """
        é‡æ’åº
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param documents: å¾…æ’åºæ–‡æ¡£åˆ—è¡¨
        :param top_k: è¿”å›ç»“æœæ•°é‡
        :return: é‡æ’åºåçš„ç»“æœåˆ—è¡¨
        """
        try:
            # å‡†å¤‡è¾“å…¥æ•°æ®
            pairs = [[query, doc] for doc in documents]
            
            # æ¨¡å‹æ¨ç†
            with torch.no_grad():
                inputs = self.rerank_tokenizer(
                    pairs,
                    padding=True,
                    truncation=True,
                    return_tensors='pt',
                    max_length=512
                ).to(self.device)
                
                scores = self.rerank_model(**inputs).logits.view(-1).float().cpu().numpy()
            
            # ç»„åˆç»“æœå¹¶æ’åº
            scored_docs = list(zip(documents, scores))
            sorted_results = sorted(scored_docs, key=lambda x: x[1], reverse=True)[:top_k]
            logging.info(f"ğŸ“Š é‡æ’åºå®Œæˆï¼šå¤„ç† {len(documents)} æ¡æ–‡æ¡£")
            return sorted_results
        except Exception as e:
            logging.error(f"âŒ é‡æ’åºå¤±è´¥: {str(e)}")
            return []
    
    def full_retrieval(self, query: str, 
                       retrieval_top_k: int = 10,
                       rerank_top_k: int = 5,
                       weights: Dict[str, float] = None) -> Dict:
        """
        å®Œæ•´RAGæ£€ç´¢æµç¨‹ï¼ˆå¤šè·¯å¬å›â†’æ··åˆæ£€ç´¢â†’é‡æ’åºï¼‰
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param retrieval_top_k: æ¯è·¯å¬å›æ•°é‡
        :param rerank_top_k: é‡æ’åºè¿”å›æ•°é‡
        :param weights: æ··åˆæ£€ç´¢æƒé‡
        :return: åŒ…å«å„é˜¶æ®µç»“æœçš„å­—å…¸
        """
        try:
            result = {}
            
            # 1. å¤šè·¯å¬å›
            multi_results = self.multi_retrieval(query, top_k=retrieval_top_k)
            result["multi_retrieval"] = multi_results
            
            # 2. æ··åˆæ£€ç´¢
            fused_results = self.hybrid_search(multi_results, weights=weights)
            result["hybrid_search"] = fused_results
            
            # 3. é‡æ’åº
            candidate_docs = [doc for doc, _ in fused_results[:rerank_top_k*2]]  # å–æ›´å¤šå€™é€‰æ–‡æ¡£
            reranked = self.rerank(query, candidate_docs, top_k=rerank_top_k)
            result["reranked"] = reranked
            
            return {
                "status": "success",
                "query": query,
                "results": result
            }
        except Exception as e:
            logging.error(f"âŒ å®Œæ•´æ£€ç´¢æµç¨‹å¤±è´¥: {str(e)}")
            return {
                "status": "error",
                "message": str(e)
            }

# ---------------------------- æµ‹è¯•ä»£ç  ----------------------------
if __name__ == "__main__":
    # æµ‹è¯•é…ç½®
    test_query = "ä»€ä¹ˆæ˜¯æ¦‚ç‡è®ºä¸æ•°ç†ç»Ÿè®¡ï¼Ÿ"
    vector_db_path = "./data/vector_db"  # éœ€æå‰æ„å»ºå¥½çš„å‘é‡åº“è·¯å¾„
    
    # åˆå§‹åŒ–æ£€ç´¢å™¨
    retriever = RAGRetriever(
        vector_db_path=vector_db_path,
        embedding_model_path="./model/embeddingmodel",
        rerank_model_name="./model/reranker",
        device="cpu",
        download_mirror="https://hf-mirror.com"
    )
    
    # æµ‹è¯•å®Œæ•´æµç¨‹
    print("\n=== å®Œæ•´RAGæµç¨‹æµ‹è¯• ===")
    full_result = retriever.full_retrieval(test_query)
    
    if full_result["status"] == "success":
        # æ‰“å°å¤šè·¯å¬å›ç»“æœ
        print("\n[å¤šè·¯å¬å›ç»“æœ]")
        for method in full_result["results"]["multi_retrieval"]:
            print(f"\n{method.upper()}ç»“æœï¼š")
            for doc, score in full_result["results"]["multi_retrieval"][method][:2]:
                print(f"[{score:.2f}] {doc[:60]}...")
        
        # æ‰“å°æ··åˆæ£€ç´¢ç»“æœ
        print("\n[æ··åˆæ£€ç´¢ç»“æœ]")
        for doc, score in full_result["results"]["hybrid_search"][:3]:
            print(f"[{score:.4f}] {doc[:60]}...")
        
        # æ‰“å°é‡æ’åºç»“æœ
        print("\n[é‡æ’åºç»“æœ]")
        for doc, score in full_result["results"]["reranked"]:
            print(f"[{score:.2f}] {doc[:60]}...")
    else:
        print(f"âŒ æ£€ç´¢å¤±è´¥: {full_result['message']}")