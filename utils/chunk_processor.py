from langchain_text_splitters import RecursiveCharacterTextSplitter
from nltk.tokenize import TextTilingTokenizer
import re
import jieba
import numpy as np
from transformers import AutoTokenizer, AutoModel
import torch
from functools import lru_cache

# åˆå§‹åŒ–æ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained("hfl/chinese-bert-wwm-ext")
model = AutoModel.from_pretrained("hfl/chinese-bert-wwm-ext").eval()
torch.set_grad_enabled(False)

# å†…ç½®åŸºç¡€ä¸­æ–‡åœç”¨è¯
BASIC_CHINESE_STOPWORDS = {
    'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'å’Œ', 'æœ‰', 'å°±', 'è¿™', 'ä¸º', 'ä¸',
    'ä¹Ÿ', 'è¦', 'å¯¹', 'éƒ½', 'è€Œ', 'åŠ', 'ç­‰', 'å¯ä»¥', 'æˆ‘', 'æˆ‘ä»¬',
    'ä»–ä»¬', 'è¿™', 'é‚£', 'ä½ ', 'æ‚¨', 'å§', 'å•Š', 'å“¦', 'å‘€', 'å•¦',
    'å—', 'å—¯', 'å”‰', 'ä¹‹', 'è€…', 'æˆ–', 'æ—¥', 'æœˆ', 'å¹´', 'ä¸ª',
    'ä¸­', 'ä¸Š', 'ä¸‹', 'å·¦', 'å³', 'å‰', 'å', 'æ—¶', 'å¾ˆ', 'å¤ª',
    'æœ€', 'æ›´', 'éå¸¸', 'ä¼š', 'æ²¡æœ‰', 'ä¸', 'æ— ', 'æœª', 'é', 'å¹¶',
    'ä½†', 'å·²', 'ç”±', 'è¢«', 'è®©', 'æŠŠ', 'å‘', 'å»', 'åˆ', 'å†'
}

class OptimizedHybridSplitter:
    def __init__(self):
        # ä¸€çº§åˆ†å‰²å™¨é…ç½®
        self.base_splitter = RecursiveCharacterTextSplitter(
            chunk_size=600,
            chunk_overlap=150,
            separators=[
                r"\n{2,}",
                r"(?<=\n\n)",
                r"[ã€‚ï¼ï¼Ÿ][â€]*",
                r";+",
                r"ï¼Œ{2,}"
            ],
            keep_separator=True
        )
        
        # äºŒçº§åˆ†å‰²å‚æ•°
        self.topic_threshold = 0.68
        self.min_section_length = 120
        
        # ä¿®å¤TextTilingTokenizerå‚æ•°é—®é¢˜
        self.tt = TextTilingTokenizer(
            w=20,  # æ­£ç¡®å‚æ•°åä¸ºwè€Œä¸æ˜¯words_per_block
            stopwords=list(BASIC_CHINESE_STOPWORDS)  # éœ€è¦è½¬æ¢ä¸ºlistç±»å‹
        )
        
        self.similarity_cache = {}
    
    # ä»¥ä¸‹æ–¹æ³•ä¿æŒä¸å˜ï¼Œä¸ä¹‹å‰ç‰ˆæœ¬ç›¸åŒ
    def _preprocess_text(self, text):
        text = re.sub(r'\r\n', '\n', text)
        text = re.sub(r'[ï¼ï¼Ÿ]', 'ã€‚', text)
        text = re.sub(r'([ï¼Œï¼›])\1+', r'\1', text)
        text = re.sub(r'([a-zA-Z])([\u4e00-\u9fa5])', r'\1 \2', text)
        return text.strip()

    @lru_cache(maxsize=5000)
    def _bert_embedding(self, text):
        inputs = tokenizer(text, 
                          return_tensors="pt", 
                          max_length=512, 
                          truncation=True)
        outputs = model(**inputs)
        return outputs.last_hidden_state.mean(dim=1).numpy()

    def _text_similarity(self, text1, text2):
        key = (text1[:100], text2[:100])
        if key not in self.similarity_cache:
            emb1 = self._bert_embedding(text1)
            emb2 = self._bert_embedding(text2)
            sim = np.dot(emb1, emb2.T) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))
            self.similarity_cache[key] = sim.item()
        return self.similarity_cache[key]

    def _chinese_texttiling(self, text):
        words = ["|".join(jieba.cut(sent)) for sent in text.split('\n')]
        return self.tt.tokenize('\n'.join(words))

    def _dynamic_split(self, chunk):
        if re.search(r'(^|\n)(\d+\.|[-*+])\s', chunk):
            return re.split(r'\n(?=\d+\.|[-*+]\s)', chunk)
            
        try:
            if len(chunk) > 800:
                tiles = self._chinese_texttiling(chunk)
                if len(tiles) > 1:
                    return tiles
        except Exception as e:
            pass
            
        sentences = self.base_splitter.split_text(chunk)
        if len(sentences) < 3:
            return [chunk]
            
        window_size = 2
        split_points = []
        for i in range(window_size, len(sentences)-window_size):
            prev = "".join(sentences[i-window_size:i])
            next_ = "".join(sentences[i:i+window_size])
            if self._text_similarity(prev, next_) < self.topic_threshold:
                split_points.append(i)
                
        merged_points = []
        prev = -1
        for p in sorted(split_points):
            if p - prev > window_size*2:
                merged_points.append(p)
                prev = p
                
        chunks = []
        start = 0
        for point in merged_points:
            chunks.append("".join(sentences[start:point]))
            start = point
        chunks.append("".join(sentences[start:]))
        
        return [c for c in chunks if len(c) >= self.min_section_length]

    def split_text(self, text):
        processed_text = self._preprocess_text(text)
        base_chunks = self.base_splitter.split_text(processed_text)
        
        final_chunks = []
        for chunk in base_chunks:
            if len(chunk) > 600:
                refined = self._dynamic_split(chunk)
                final_chunks.extend(refined)
            else:
                final_chunks.append(chunk)
                
        merged = []
        buffer = ""
        for c in final_chunks:
            if len(buffer) + len(c) < 300:
                buffer += "\n" + c
            else:
                if buffer:
                    merged.append(buffer)
                buffer = c
        if buffer:
            merged.append(buffer)
            
        return merged

# æµ‹è¯•ä»£ç ä¿æŒä¸å˜
def test_hybrid_chunker():
    splitter = OptimizedHybridSplitter()
    
    test_text = """
ã€Šé‡‘é“²é“²ä¹‹æˆ˜ã€‹4.3ç‰ˆæœ¬æ›´æ–°å…¬å‘Š

ç¬¬ä¸€ç«  æ ¸å¿ƒæœºåˆ¶è°ƒæ•´

1.1 ç»æµç³»ç»Ÿé‡æ„

æœ¬æ¬¡æ›´æ–°å½»åº•é‡åšç»æµç³»ç»Ÿï¼Œå…³é”®æ”¹åŠ¨åŒ…æ‹¬ï¼š
â€¢ åŸºç¡€é‡‘å¸æ”¶ç›Šä»5/å›åˆ â†’ 6/å›åˆ
â€¢ æ–°å¢åˆ©æ¯å€ç‡æœºåˆ¶ï¼š
  - ç­‰çº§5ä»¥ä¸‹ï¼šæ¯10é‡‘å¸+1/å›åˆ
  - ç­‰çº§6-8ï¼šæ¯10é‡‘å¸+2/å›åˆ
  - ç­‰çº§9ï¼šæ¯10é‡‘å¸+3/å›åˆ
â€¢ è¿èƒœ/è¿è´¥å¥–åŠ±å…¬å¼è°ƒæ•´ï¼š
  å…¬å¼ï¼šå¥–åŠ± = åŸºç¡€å€¼ Ã— (1 + 0.2Ã—å›åˆæ•°)
  ç¤ºä¾‹ï¼š3è¿èƒœæ—¶è·å¾— 2Ã—1.6=3.2é‡‘å¸ï¼ˆå‘ä¸‹å–æ•´ï¼‰

1.2 è£…å¤‡åˆæˆç³»ç»Ÿ

æ–°å¢è£…å¤‡åˆæˆå¼‚å¸¸æ£€æµ‹æœºåˆ¶ï¼Œè§£å†³ä»¥ä¸‹é—®é¢˜ï¼š
1. å½“åŒæ—¶å­˜åœ¨ã€æš´é£å¤§å‰‘+å¥³ç¥æ³ªã€‘å’Œã€å¥³ç¥æ³ª+æš´é£å¤§å‰‘ã€‘æ—¶
2. åˆæˆä¼˜å…ˆçº§é—®é¢˜ï¼ˆä¿®å¤æ—¶é—´æˆ³å†²çªBUGï¼‰
3. è§†è§‰ç‰¹æ•ˆä¸å®é™…æ•ˆæœä¸åŒæ­¥é—®é¢˜

ç¬¬äºŒç«  è‹±é›„ä¸ç¾ç»Š

2.1 è‹±é›„æ•°å€¼è°ƒæ•´è¡¨

è‹±é›„åç§° | è´¹ç”¨ | ç”Ÿå‘½å€¼ | æŠ€èƒ½ä¼¤å®³
-------|-----|-------|---------
é˜¿ç‹¸   | 4   | 850 â†’ 800 | 350/500/800 â†’ 320/480/750
æé’   | 2   | 750 â†’ 800 | çœ©æ™•æ—¶é—´1.2s â†’ 1.5s
ç´¢å°”   | 5   | 1000 â†’ 950 | é»‘æ´èŒƒå›´+15%

2.2 ç¾ç»Šæ•ˆæœé‡åš

ã€ç¥è°•è€…ã€‘æ•ˆæœå˜æ›´ï¼š
æ—§ï¼šå…¨ä½“+40é­”æŠ—
æ–°ï¼šæ¯3ç§’å›å¤10/20/35æ³•åŠ›å€¼

ã€æ³•å¸ˆã€‘æ•ˆæœå¢å¼ºï¼š
æ³•æœ¯å¼ºåº¦åŠ æˆä»20/45/80% â†’ 25/50/90%

ç¬¬ä¸‰ç«  ç©å®¶ç”Ÿæ€

3.1 ç¤¾åŒºåé¦ˆåˆ†æ

ä»50ä¸‡å±€å¯¹æˆ˜æ•°æ®ä¸­å‘ç°ï¼š
- é˜¿ç‹¸ä½¿ç”¨ç‡ä¸‹é™22%ï¼Œä½†èƒœç‡æå‡4.3%
- æ–°ç»æµç³»ç»Ÿä½¿å¹³å‡å‡9çº§æ—¶é—´æå‰1.2å›åˆ
- è£…å¤‡å¼‚å¸¸åé¦ˆå‡å°‘78%

ç©å®¶"äº‘é¡¶é­”æœ¯å¸ˆ"è¯„è®ºï¼š
"æ–°çš„åˆ©æ¯æœºåˆ¶è®©è¿è¥ç­–ç•¥æ›´æœ‰æ·±åº¦ï¼Œä½†æé’çš„å¢å¼ºè®©å‰æœŸé˜µå®¹è¿‡äºå¼ºåŠ¿ï¼Œ
å»ºè®®å°†ç”Ÿå‘½å€¼å›è°ƒåˆ°780å·¦å³ä¿æŒå¹³è¡¡ã€‚"

3.2 èµ›äº‹ä½“ç³»å‡çº§

2024å…¨çƒæ€»å†³èµ›èµ›åˆ¶æ›´æ–°ï¼š
1. å°ç»„èµ›é˜¶æ®µï¼š
   - BO3åŒè´¥åˆ¶
   - ç¦ç”¨è‹±é›„æ± ç³»ç»Ÿ
2. å†³èµ›é˜¶æ®µæ–°å¢ã€Œå·…å³°å¯¹å†³ã€æ¨¡å¼ï¼š
   åŒæ–¹ä½¿ç”¨å®Œå…¨ç›¸åŒéšæœºé˜µå®¹

æŠ€æœ¯å›¢é˜Ÿé€éœ²æ­£åœ¨å¼€å‘ï¼š
- å®æ—¶èƒœç‡é¢„æµ‹ç³»ç»Ÿï¼ˆåŸºäºLSTMç¥ç»ç½‘ç»œï¼‰
- è·¨å¹³å°å¯¹æˆ˜åŠŸèƒ½ï¼ˆé¢„è®¡2024Q2ä¸Šçº¿ï¼‰
"""

    chunks = splitter.split_text(test_text)
    
    print("\nğŸ” æ·±åº¦æµ‹è¯•åˆ†å—ç»“æœï¼š")
    print(f"æ€»å­—ç¬¦æ•°ï¼š{len(test_text)}")
    print(f"æœ€ç»ˆåˆ†å—æ•°ï¼š{len(chunks)}")
    for i, chunk in enumerate(chunks):
        print(f"\nâ–‡â–‡ å—{i+1} ({len(chunk)}å­—ç¬¦) â–‡â–‡")
        print(chunk[:100] + "..." if len(chunk) > 100 else chunk)


if __name__ == "__main__":
    test_hybrid_chunker()