# rag_retriever.py - RAGæ£€ç´¢æ¨¡å—ï¼ˆå¤šè·¯å¬å›+æ··åˆæ£€ç´¢+é‡æ’åºï¼‰
import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'  # è®¾ç½®é•œåƒæº

import logging
from typing import List, Dict, Tuple
import numpy as np
from pathlib import Path

# ç¬¬ä¸‰æ–¹åº“
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from rank_bm25 import BM25Okapi
from transformers import AutoModelForSequenceClassification, AutoTokenizer
import torch

class RAGRetriever:
    """RAGæ£€ç´¢å™¨ï¼ˆæ”¯æŒå¤šè·¯å¬å›ã€æ··åˆæ£€ç´¢ã€é‡æ’åºï¼‰"""
    
    def __init__(
        self,
        vector_db_path: str,
        embedding_model_path: str = "./model/embeddingmodel",
        rerank_model_name: str = "BAAI/bge-reranker-base",
        device: str = "cpu"
    ):
        """
        åˆå§‹åŒ–æ£€ç´¢å™¨
        :param vector_db_path: å‘é‡æ•°æ®åº“è·¯å¾„
        :param embedding_model_path: æœ¬åœ°åµŒå…¥æ¨¡å‹è·¯å¾„
        :param rerank_model_name: é‡æ’åºæ¨¡å‹åç§°
        :param device: è®¡ç®—è®¾å¤‡
        """
        self.device = device
        self._init_logging()
        
        # åˆå§‹åŒ–åŸºç¡€ç»„ä»¶
        self._load_embedding_model(embedding_model_path)
        self._load_vector_db(vector_db_path)
        self._load_rerank_model(rerank_model_name)
        
        # åˆå§‹åŒ–BM25
        self._init_bm25()

    def _init_logging(self):
        """é…ç½®æ—¥å¿—è®°å½•"""
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(levelname)s - %(module)s - %(message)s"
        )
    
    def _load_embedding_model(self, model_path: str):
        """åŠ è½½æœ¬åœ°åµŒå…¥æ¨¡å‹"""
        try:
            self.embedding_model = HuggingFaceEmbeddings(
                model_name=model_path,
                model_kwargs={'device': self.device},
                encode_kwargs={'normalize_embeddings': False}
            )
            logging.info("âœ… åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            logging.error(f"âŒ åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            raise
    
    def _load_vector_db(self, db_path: str):
        """åŠ è½½å‘é‡æ•°æ®åº“"""
        try:
            self.vector_db = FAISS.load_local(
                folder_path=db_path,
                embeddings=self.embedding_model,
                allow_dangerous_deserialization=True
            )
            logging.info(f"âœ… å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸï¼ˆ{self.vector_db.index.ntotal}æ¡æ•°æ®ï¼‰")
        except Exception as e:
            logging.error(f"âŒ å‘é‡æ•°æ®åº“åŠ è½½å¤±è´¥: {str(e)}")
            raise
    
    def _load_rerank_model(self, model_name: str):
        """åŠ è½½é‡æ’åºæ¨¡å‹"""
        try:
            self.rerank_tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.rerank_model = AutoModelForSequenceClassification.from_pretrained(model_name)
            self.rerank_model.to(self.device)
            logging.info(f"âœ… é‡æ’åºæ¨¡å‹åŠ è½½æˆåŠŸ: {model_name}")
        except Exception as e:
            logging.error(f"âŒ é‡æ’åºæ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            raise
    
    def _init_bm25(self):
        """åˆå§‹åŒ–BM25ç´¢å¼•"""
        try:
            all_texts = [doc.page_content for doc in self.vector_db.docstore._dict.values()]
            self.bm25_index = BM25Okapi([doc.split() for doc in all_texts])
            self.all_texts = all_texts
            logging.info(f"âœ… BM25ç´¢å¼•æ„å»ºæˆåŠŸï¼ˆ{len(all_texts)}æ¡æ•°æ®ï¼‰")
        except Exception as e:
            logging.error(f"âŒ BM25ç´¢å¼•æ„å»ºå¤±è´¥: {str(e)}")
            raise

    def multi_retrieval(self, query: str, top_k: int = 10) -> Dict[str, List[Tuple[str, float]]]:
        """
        å¤šè·¯å¬å›æ£€ç´¢
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param top_k: æ¯è·¯å¬å›æ•°é‡
        :return: å„è·¯çš„æ£€ç´¢ç»“æœå­—å…¸
        """
        results = {}
        
        # å‘é‡æ£€ç´¢
        vector_results = self.vector_db.similarity_search_with_score(query, k=top_k)
        results["vector"] = [(doc.page_content, score) for doc, score in vector_results]
        
        # BM25æ£€ç´¢
        tokenized_query = query.split()
        bm25_scores = self.bm25_index.get_scores(tokenized_query)
        bm25_indices = np.argsort(bm25_scores)[::-1][:top_k]
        results["bm25"] = [(self.all_texts[i], bm25_scores[i]) for i in bm25_indices]
        
        logging.info(f"ğŸ” å¤šè·¯å¬å›å®Œæˆï¼šå‘é‡å¬å› {len(results['vector'])} æ¡ï¼ŒBM25å¬å› {len(results['bm25'])} æ¡")
        return results

    def hybrid_search(self, retrieval_results: Dict[str, List[Tuple[str, float]]], 
                      weights: Dict[str, float] = None) -> List[Tuple[str, float]]:
        """
        æ··åˆæ£€ç´¢ï¼ˆåŠ æƒèåˆï¼‰
        :param retrieval_results: å¤šè·¯å¬å›ç»“æœ
        :param weights: å„æ£€ç´¢æ–¹æ³•çš„æƒé‡ï¼ˆé»˜è®¤ï¼šå‘é‡0.6ï¼ŒBM25 0.4ï¼‰
        :return: èåˆåçš„ç»“æœåˆ—è¡¨
        """
        # è®¾ç½®é»˜è®¤æƒé‡
        default_weights = {"vector": 0.6, "bm25": 0.4}
        weights = weights or default_weights
        
        # ç»“æœèåˆ
        fused_results = {}
        for method, docs in retrieval_results.items():
            for doc, score in docs:
                # åˆ†æ•°å½’ä¸€åŒ–å¤„ç†
                norm_score = 1 / (1 + np.exp(-score)) if method == "vector" else score
                fused_score = norm_score * weights.get(method, 0)
                if doc in fused_results:
                    fused_results[doc] += fused_score
                else:
                    fused_results[doc] = fused_score
        
        # æ’åºå¹¶è¿”å›
        sorted_results = sorted(fused_results.items(), key=lambda x: x[1], reverse=True)
        logging.info(f"ğŸ§¬ æ··åˆæ£€ç´¢å®Œæˆï¼šèåˆ {len(sorted_results)} æ¡ç»“æœ")
        return sorted_results

    def rerank(self, query: str, documents: List[str], top_k: int = 5) -> List[Tuple[str, float]]:
        """
        é‡æ’åº
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param documents: å¾…æ’åºæ–‡æ¡£åˆ—è¡¨
        :param top_k: è¿”å›ç»“æœæ•°é‡
        :return: é‡æ’åºåçš„ç»“æœåˆ—è¡¨
        """
        try:
            # å‡†å¤‡è¾“å…¥æ•°æ®
            pairs = [[query, doc] for doc in documents]
            
            # æ¨¡å‹æ¨ç†
            with torch.no_grad():
                inputs = self.rerank_tokenizer(
                    pairs,
                    padding=True,
                    truncation=True,
                    return_tensors='pt',
                    max_length=512
                ).to(self.device)
                
                scores = self.rerank_model(**inputs).logits.view(-1).float().cpu().numpy()
            
            # ç»„åˆç»“æœå¹¶æ’åº
            scored_docs = list(zip(documents, scores))
            sorted_results = sorted(scored_docs, key=lambda x: x[1], reverse=True)[:top_k]
            logging.info(f"ğŸ“Š é‡æ’åºå®Œæˆï¼šå¤„ç† {len(documents)} æ¡æ–‡æ¡£")
            return sorted_results
        except Exception as e:
            logging.error(f"âŒ é‡æ’åºå¤±è´¥: {str(e)}")
            return []

# ---------------------------- æµ‹è¯•ä»£ç  ----------------------------
if __name__ == "__main__":
    # æµ‹è¯•é…ç½®
    test_query = "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"
    vector_db_path = "./data/vector_db"  # éœ€æå‰æ„å»ºå¥½çš„å‘é‡åº“è·¯å¾„
    
    # åˆå§‹åŒ–æ£€ç´¢å™¨
    retriever = RAGRetriever(
        vector_db_path=vector_db_path,
        embedding_model_path="./model/embeddingmodel",
        rerank_model_name="BAAI/bge-reranker-base",
        device="cpu"
    )
    
    # å¤šè·¯å¬å›æµ‹è¯•
    print("\n=== å¤šè·¯å¬å›æµ‹è¯• ===")
    multi_results = retriever.multi_retrieval(test_query, top_k=3)
    for method in multi_results:
        print(f"\n{method.upper()}ç»“æœï¼š")
        for doc, score in multi_results[method][:2]:  # å±•ç¤ºå‰2ä¸ªç»“æœ
            print(f"[{score:.2f}] {doc[:60]}...")
    
    # æ··åˆæ£€ç´¢æµ‹è¯•
    print("\n=== æ··åˆæ£€ç´¢æµ‹è¯• ===")
    fused_results = retriever.hybrid_search(multi_results)
    for doc, score in fused_results[:3]:
        print(f"[{score:.4f}] {doc[:60]}...")
    
    # é‡æ’åºæµ‹è¯•
    print("\n=== é‡æ’åºæµ‹è¯• ===")
    candidate_docs = [doc for doc, _ in fused_results[:6]]  # å–å‰6ä¸ªå€™é€‰æ–‡æ¡£
    reranked_results = retriever.rerank(test_query, candidate_docs)
    for doc, score in reranked_results:
        print(f"[{score:.2f}] {doc[:60]}...")